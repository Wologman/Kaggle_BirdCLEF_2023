{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## [BirdCLEF2023](https://www.kaggle.com/competitions/birdclef-2023)\n\n\n[Inference notebook](https://www.kaggle.com/code/ollypowell/birdclef-2023-pytorch-lightning-inference) \n\nThis one forked from V44.  Refer to training version 17 for a working version to troubleshoot.  \nExpect AP score = 0.62 after epoch1\n\nOriginal Forks: [Generate spectrograms](https://www.kaggle.com/code/nischaydnk/split-creating-melspecs-stage-1), [Training](https://www.kaggle.com/code/nischaydnk/birdclef-2023-pytorch-lightning-training-w-cmap), [Inference](https://www.kaggle.com/code/nischaydnk/birdclef-2023-pytorch-lightning-inference)\n\n### Strategy\n\nGet this working again.  Use to pre-train\nApply the fix to the training notebook, but start that from the weights from this one, with one extra layer\nRun this on a larger dataset from the 3 competitions.\n\nKaggle Strategy: \n- Strong single model, Tweak single-model hyperparameters and augmentation & sampling recipe a little, backbone fine tuning, stronger classifier head, try weighted sampling\n- Develop a pre-training model with the full 2021, 2022, 2023 data, to help with generalisation.  Find tune the backbone. Then leave that fixed, just keep the weights for a seperate notebook\n- Fork this notebook and fine-tune with the previous weights, but 2023 data and classes only\n- Develop two seperate classifier heads for rare and common birds, by working with the data, loss function, weighted sampling\n- Ensemble those two heads together and see if I get some improvement within the competition constraints\n- Post processing","metadata":{"papermill":{"duration":6.667352,"end_time":"2022-04-22T06:00:08.901647","exception":false,"start_time":"2022-04-22T06:00:02.234295","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-13T02:21:19.549902Z","iopub.execute_input":"2023-03-13T02:21:19.55051Z","iopub.status.idle":"2023-03-13T02:21:19.579716Z","shell.execute_reply.started":"2023-03-13T02:21:19.550465Z","shell.execute_reply":"2023-03-13T02:21:19.578176Z"}}},{"cell_type":"code","source":"!pip install -q torchtoolbox timm colorednoise\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport gc\nimport warnings\nimport random \nfrom pathlib import Path\nimport plotly.express as px\nimport pandas as pd\nimport plotly.io as pio\n\n\n# Torch and PyTorch\nimport torch\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer\nimport torch.multiprocessing as mp\nfrom pytorch_lightning.callbacks import ModelCheckpoint, BackboneFinetuning, EarlyStopping\nimport torch.nn as nn\nimport torch.multiprocessing as mp\nfrom torch.nn.functional import cross_entropy, binary_cross_entropy_with_logits\nfrom torch.utils.data import Dataset, DataLoader \nfrom torch.utils.data.sampler import Sampler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts, ReduceLROnPlateau # OneCycleLR\nimport torchmetrics\nimport timm\n\n#ML Modules\nfrom sklearn import model_selection\nfrom sklearn.model_selection import StratifiedKFold\nimport sklearn.metrics as skm\nimport torchvision.transforms as transforms\nimport torchvision.io \nimport albumentations as A\n\n#Audio \nimport scipy.signal as sps\nimport torchaudio\nimport librosa\nfrom PIL import Image\nimport colorednoise as cn","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:56:23.844039Z","iopub.execute_input":"2023-04-29T00:56:23.844873Z","iopub.status.idle":"2023-04-29T00:56:52.753836Z","shell.execute_reply.started":"2023-04-29T00:56:23.844821Z","shell.execute_reply":"2023-04-29T00:56:52.75226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pl.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:56:52.760068Z","iopub.execute_input":"2023-04-29T00:56:52.762607Z","iopub.status.idle":"2023-04-29T00:56:52.772379Z","shell.execute_reply.started":"2023-04-29T00:56:52.762549Z","shell.execute_reply":"2023-04-29T00:56:52.771222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    #Training Parameters\n    EXPERIMENT = 23 # Increased LR\n    WEIGHTED_SAMPLING = False\n    NUM_WORKERS = 8\n    NUM_CLASSES = 264\n    BATCH_SIZE = 64\n    EPOCHS = 12\n    PRECISION = 16    \n    PATIENCE = 3\n    MIN_DELTA = 0\n    SEED = 2023\n    MODEL = \"tf_efficientnetv2_s_in21k\"\n    PRETRAINED = True            \n    WEIGHT_DECAY = 1e-3\n    LR = 1e-4\n    USE_MIXUP= True\n    MIXUP_ALPHA = 0.2   \n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    LOSS_FUNCTION = nn.CrossEntropyLoss()  #nn.BCEWithLogitsLoss() \n    IMAGE_SIZE = 256\n    \n    #Train and CV Parameters\n    N_FOLDS = 10 \n    RARE_THRESHOLD = 4 # Samples with less than this many values will not be allowed in validation\n\n    #Filepaths\n    OUT_DIR = '/kaggle/working'\n    LABELS_PATH = \"/kaggle/input/birdclef23-train-8-sec-wav/train_23_cropped.csv\"\n    BACKGROUND_NOISE_FLDR = '/kaggle/input/birdclef23-uniform-noise-chunks/birdclef23-backgrounds'\n\n#os.listdir(Config.BACKGROUND_NOISE_FLDR)","metadata":{"papermill":{"duration":0.099568,"end_time":"2022-04-22T06:00:18.542447","exception":false,"start_time":"2022-04-22T06:00:18.442879","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-29T00:56:52.777512Z","iopub.execute_input":"2023-04-29T00:56:52.778317Z","iopub.status.idle":"2023-04-29T00:56:52.861484Z","shell.execute_reply.started":"2023-04-29T00:56:52.778278Z","shell.execute_reply":"2023-04-29T00:56:52.860288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"  class Audio:\n    SR = 32000\n    DURATION = 5  # Duration the loaded wav file will be cropped to.\n    CHUNK_LENGTH = 8 # Maximum duration of the wav files\n    N_MELS = 128 # Try increasing this to 256 later, once other experiments tried\n    FMIN = 20\n    FMAX = 14000\n    WINDOW = 1024  \n    HOP_LENGTH = 312\n    N_FFT = 1024\n    \n    \n    # last years 3rd place: two strategies\n    # sr: 32000, window_size: 2048, hop_size: 1024, fmin: 200, fmax: 14000, mel_bins: 224\n    # sr: 32000, window_size: 1024, hop_size: 512, fmin: 50, fmax: 14000, mel_bins: 128","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:56:52.866178Z","iopub.execute_input":"2023-04-29T00:56:52.866876Z","iopub.status.idle":"2023-04-29T00:56:52.879869Z","shell.execute_reply.started":"2023-04-29T00:56:52.866837Z","shell.execute_reply":"2023-04-29T00:56:52.878538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setup","metadata":{}},{"cell_type":"code","source":"pl.seed_everything(Config.SEED, workers=True)\ntorch.set_flush_denormal(True)\npd.set_option('display.max_colwidth', None)\nwarnings.filterwarnings('ignore')\nsave_path = Path(Config.OUT_DIR) / f'Exp{Config.EXPERIMENT}'\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\naccelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\nConfig.PRECISION = 16 if accelerator == 'gpu' else 32\nnoise = Path(Config.BACKGROUND_NOISE_FLDR)\nAudio.BACKGROUNDS = [noise / p for p in noise.rglob('*.wav')]\nf'There are {len(Audio.BACKGROUNDS)} Audio background files (each made up of concatenated 5 second random clips)'","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:56:52.88201Z","iopub.execute_input":"2023-04-29T00:56:52.88256Z","iopub.status.idle":"2023-04-29T00:56:52.94886Z","shell.execute_reply.started":"2023-04-29T00:56:52.882513Z","shell.execute_reply":"2023-04-29T00:56:52.947717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the train and validation splits using stratified-k-fold to keep the values representative of their class proportions. Later on when the differences are getting smaller and I'm choosing between options, base the CV scheme on 5 folds mean + one standard deviation.  This is too computationaly expensive for now.","metadata":{}},{"cell_type":"code","source":"in_df = pd.read_csv(Config.LABELS_PATH)\nConfig.NUM_CLASSES = len(in_df.primary_label.unique())\nprint(f'Training + Validation with {in_df.shape[0]} audio samples')\nprint(f'There are {Config.NUM_CLASSES} primary class labels')\nin_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:56:52.950288Z","iopub.execute_input":"2023-04-29T00:56:52.950981Z","iopub.status.idle":"2023-04-29T00:56:53.159556Z","shell.execute_reply.started":"2023-04-29T00:56:52.95094Z","shell.execute_reply":"2023-04-29T00:56:53.158464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Temporarily drop any super rare classes from the dataframe, so they don't end up loosing precious samples from training to the validation split.","metadata":{}},{"cell_type":"code","source":"mask = in_df['primary_label'].map(in_df['primary_label'].value_counts()) > Config.RARE_THRESHOLD\ncommon_df = in_df[mask][['primary_label', 'filepath']]\nmask = in_df['primary_label'].map(in_df['primary_label'].value_counts()) <= Config.RARE_THRESHOLD\nrare_df = in_df[mask][['primary_label', 'filepath']]\nrare_df.primary_label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:56:53.164145Z","iopub.execute_input":"2023-04-29T00:56:53.166527Z","iopub.status.idle":"2023-04-29T00:56:53.206633Z","shell.execute_reply.started":"2023-04-29T00:56:53.166487Z","shell.execute_reply":"2023-04-29T00:56:53.205538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Temporarily removing {rare_df.shape[0]} rare instances from the dataset before splitting because'\n      f' they have less than or equal to {Config.RARE_THRESHOLD} audio samples per class')","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:56:53.211664Z","iopub.execute_input":"2023-04-29T00:56:53.214176Z","iopub.status.idle":"2023-04-29T00:56:53.22413Z","shell.execute_reply.started":"2023-04-29T00:56:53.214137Z","shell.execute_reply":"2023-04-29T00:56:53.222903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf =StratifiedKFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=Config.SEED)\ntarget = common_df['primary_label'] \n\nfor train_index, val_index in skf.split(common_df, target):\n    train_df, val_df = common_df.iloc[train_index], common_df.iloc[val_index]\ntrain_df = pd.concat([train_df, rare_df])\n    \nprint(f'The training dataframe has {train_df.shape[0]} rows\\n'\n      f'The validation dataframe has {val_df.shape[0]} rows')","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:56:53.229235Z","iopub.execute_input":"2023-04-29T00:56:53.229868Z","iopub.status.idle":"2023-04-29T00:56:53.300156Z","shell.execute_reply.started":"2023-04-29T00:56:53.229832Z","shell.execute_reply":"2023-04-29T00:56:53.299073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specifically check the filename column\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:56:53.308262Z","iopub.execute_input":"2023-04-29T00:56:53.310622Z","iopub.status.idle":"2023-04-29T00:56:53.32771Z","shell.execute_reply.started":"2023-04-29T00:56:53.31058Z","shell.execute_reply":"2023-04-29T00:56:53.326547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create & Fill birds with 0 samples in validation.  This is only needed because of the get-dummies approach with the dataframes.  The validation df misses out on columns for the missing species.  I also considered up-sampling the rare species, and ensuring one of the extra samples from each would go into the validation dataframe, but this would likely result in over-estimating the validation performance, and potentially over-fitting.","metadata":{}},{"cell_type":"code","source":"train_df = pd.concat([train_df, pd.get_dummies(train_df['primary_label'])], axis=1)\nval_df = pd.concat([val_df, pd.get_dummies(val_df['primary_label'])], axis=1)\ntrain_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:56:53.332099Z","iopub.execute_input":"2023-04-29T00:56:53.334608Z","iopub.status.idle":"2023-04-29T00:56:53.398366Z","shell.execute_reply.started":"2023-04-29T00:56:53.334568Z","shell.execute_reply":"2023-04-29T00:56:53.397131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"birds = list(train_df.primary_label.unique())\nmissing_birds = list(set(list(train_df.primary_label.unique())).difference(list(val_df.primary_label.unique())))\nnon_missing_birds = list(set(list(train_df.primary_label.unique())).difference(missing_birds))\nval_df[missing_birds] = 0\nval_df = val_df[train_df.columns] ## Fix order\nlen(non_missing_birds)\nval_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:56:53.40104Z","iopub.execute_input":"2023-04-29T00:56:53.40199Z","iopub.status.idle":"2023-04-29T00:56:53.452199Z","shell.execute_reply.started":"2023-04-29T00:56:53.401929Z","shell.execute_reply":"2023-04-29T00:56:53.451124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Helper functions and classes","metadata":{}},{"cell_type":"code","source":"def load_sf(wav_path, sr=Audio.SR):\n    y, _ = librosa.load(wav_path, sr=sr)\n    return y\n\n\ndef compute_pcen(y):\n    melspec = librosa.feature.melspectrogram(y=y, sr=Audio.SR, n_mels=Audio.N_MELS, n_fft= Audio.N_FFT, \n                                             hop_length = Audio.HOP_LENGTH, fmin=Audio.FMIN, fmax=Audio.FMAX)\n    pcen = librosa.pcen(melspec, sr=Audio.SR, gain=0.98, bias=2, power=0.5, time_constant=0.4, eps=0.000001).astype(np.float32)\n    return pcen\n\n\ndef compute_melspec(y):\n    melspec = librosa.feature.melspectrogram(y=y, sr=Audio.SR, n_mels=Audio.N_MELS, n_fft=Audio.N_FFT, \n                                             hop_length = Audio.HOP_LENGTH, fmin=Audio.FMIN, fmax=Audio.FMAX)\n    melspec = librosa.power_to_db(melspec)  #.astype(np.float32)\n    return melspec\n\n\ndef mono_to_color(X, eps=1e-6, mean=None, std=None):\n    _min, _max = X.min(), X.max()\n    if (_max - _min) > eps:\n        X = (X - _min) / (_max - _min) #scales to a range of [0,1]\n        X = X.astype(np.float32)\n    else:\n        X = np.zeros_like(X, dtype=np.float32)\n    return X\n\n\ndef crop_or_pad(y, length, train='train'):\n    y = np.concatenate([y, y, y])\n    if len(y) <= length:\n        y = np.concatenate([y, np.zeros(length - len(y))])\n    else:\n        if train != 'train':\n            start = 0\n        else:\n            start = np.random.randint(len(y) - length)\n        y = y[start: start + length]\n    return y\n\n\ndef random_crop(arr, length):\n    start = np.random.randint(len(arr) - length)\n    arr = arr[start: start + length]\n    return arr\n\n\ndef reshape_image(arr):  #chop the image in half along the long dimension and stack to make more square\n    cols = arr.shape[1]//2 \n    remainder = arr.shape[1] % 2\n    half1 = arr[:, :cols + remainder]\n    half2 = arr[:, cols:]\n    \n    if np.random.choice([True,False], size=1)[0]:\n        arr =  np.vstack((half1, half2))\n    else:\n        arr = np.vstack((half2, half1))\n    return arr","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:56:53.456604Z","iopub.execute_input":"2023-04-29T00:56:53.459103Z","iopub.status.idle":"2023-04-29T00:56:53.480363Z","shell.execute_reply.started":"2023-04-29T00:56:53.459047Z","shell.execute_reply":"2023-04-29T00:56:53.479235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Compose:\n    def __init__(self, transforms: list):\n        self.transforms = transforms\n\n    def __call__(self, y: np.ndarray, sr):\n        for trns in self.transforms:\n            y = trns(y, sr)\n        return y\n\n\nclass AudioTransform:\n    def __init__(self, always_apply=False, p=0.5):\n        self.always_apply = always_apply\n        self.p = p\n\n    def __call__(self, y: np.ndarray, sr):\n        if self.always_apply:\n            return self.apply(y, sr=sr)\n        else:\n            if np.random.rand() < self.p:\n                return self.apply(y, sr=sr)\n            else:\n                return y\n\n    def apply(self, y: np.ndarray, **params):\n        raise NotImplementedError\n\n\nclass OneOf(Compose):\n    def __init__(self, transforms, p=0.5):\n        super().__init__(transforms)\n        self.p = p\n        transforms_ps = [t.p for t in transforms]\n        s = sum(transforms_ps)\n        self.transforms_ps = [t / s for t in transforms_ps]\n\n    def __call__(self, y: np.ndarray, sr):\n        data = y\n        if self.transforms_ps and (np.random.random() < self.p):\n            random_state = np.random.RandomState(np.random.randint(0, 2 ** 32 - 1))\n            t = random_state.choice(self.transforms, p=self.transforms_ps)\n            data = t(y, sr)\n        return data\n\n\nclass Normalize(AudioTransform):\n    def __init__(self, always_apply=True, p=1):\n        super().__init__(always_apply, p)\n\n    def apply(self, y: np.ndarray, **params):\n        max_vol = np.abs(y).max()\n        y_vol = y * 1 / max_vol\n        return np.asfortranarray(y_vol)\n\n\nclass NoiseInjection(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, max_noise_level=0.5):\n        super().__init__(always_apply, p)\n\n        self.noise_level = (0.0, max_noise_level)\n\n    def apply(self, y: np.ndarray, **params):\n        noise_level = np.random.uniform(*self.noise_level)\n        noise = np.random.randn(len(y))\n        augmented = (y + noise * noise_level).astype(y.dtype)\n        return augmented\n\n\nclass GaussianNoise(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20):\n        super().__init__(always_apply, p)\n\n        self.min_snr = min_snr\n        self.max_snr = max_snr\n\n    def apply(self, y: np.ndarray, **params):\n        snr = np.random.uniform(self.min_snr, self.max_snr)\n        a_signal = np.sqrt(y ** 2).max()\n        a_noise = a_signal / (10 ** (snr / 20))\n\n        white_noise = np.random.randn(len(y))\n        a_white = np.sqrt(white_noise ** 2).max()\n        augmented = (y + white_noise * 1 / a_white * a_noise).astype(y.dtype)\n        return augmented\n\n#https://github.com/felixpatzelt/colorednoise\nclass PinkNoise(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20):\n        super().__init__(always_apply, p)\n\n        self.min_snr = min_snr\n        self.max_snr = max_snr\n\n    def apply(self, y: np.ndarray, **params):\n        snr = np.random.uniform(self.min_snr, self.max_snr)\n        a_signal = np.sqrt(y ** 2).max()\n        a_noise = a_signal / (10 ** (snr / 20))\n\n        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))\n        a_pink = np.sqrt(pink_noise ** 2).max()\n        augmented = (y + pink_noise * 1 / a_pink * a_noise).astype(y.dtype)\n        return augmented\n\n\nclass BrownNoise(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20):\n        super().__init__(always_apply, p)\n\n        self.min_snr = min_snr\n        self.max_snr = max_snr\n\n    def apply(self, y: np.ndarray, **params):\n        snr = np.random.uniform(self.min_snr, self.max_snr)\n        a_signal = np.sqrt(y ** 2).max()\n        a_noise = a_signal / (10 ** (snr / 20))\n\n        brown_noise = cn.powerlaw_psd_gaussian(2, len(y))\n        a_brown = np.sqrt(brown_noise ** 2).max()\n        augmented = (y + brown_noise * 1 / a_brown * a_noise).astype(y.dtype)\n        return augmented\n\n    \n#https://www.kaggle.com/code/hidehisaarai1213/rfcx-audio-data-augmentation-japanese-english\n#https://medium.com/@makcedward/data-augmentation-for-audio-76912b01fdf6\nclass AddBackround(AudioTransform):\n    def __init__(self, always_apply=False, p=0.6, min_snr=1, max_snr=20, background_pths=Audio.BACKGROUNDS, sr=Audio.SR, duration=Audio.DURATION):\n        super().__init__(always_apply, p)\n        self.min_snr = min_snr\n        self.max_snr = max_snr\n        self.back_pths = Audio.BACKGROUNDS\n        self.background = load_sf(random.choice(Audio.BACKGROUNDS))\n        self.d_len = duration * sr\n\n    def apply(self, y: np.ndarray, **params):\n        snr = np.random.uniform(self.min_snr, self.max_snr)\n        if random.random() < 0.05:  #load a new background file roughly every 20th sample\n            background_path = random.choice(Audio.BACKGROUNDS)\n            #print(background_path)\n            self.background = load_sf(background_path)\n        background = random_crop(self.background, self.d_len)\n        a_signal = np.sqrt(y ** 2).max()\n        a_noise = a_signal / (10 ** (snr / 20))  \n        l_signal = len(y)\n\n        a_background = np.sqrt(background ** 2).max()\n        l_background = len(background)\n\n        if l_signal > l_background:\n            ratio = l_signal//l_background\n            background = np.tile(background, ratio+1 )\n            background = background[0:l_signal]\n\n        if l_signal < l_background:    \n            background = background[0:l_signal]\n\n        augmented = (y + background * 1 / a_background * a_noise).astype(y.dtype)\n        return augmented\n    \nclass SpecAugment:\n    def __init__(self, time_drop_width=80, time_stripes_num=2, freq_drop_width=12, freq_stripes_num=2):\n        self.time_drop_width = time_drop_width\n        self.time_stripes_num = time_stripes_num\n        self.freq_drop_width = freq_drop_width\n        self.freq_stripes_num = freq_stripes_num\n        \n    def __call__(self, spec):\n        time_bins, freq_bins = spec.shape\n        \n        # Time masking\n        for i in range(self.time_stripes_num):\n            start = np.random.randint(0, time_bins - self.time_drop_width)\n            spec[start:start+self.time_drop_width, :] = 0\n        \n        # Frequency masking\n        for i in range(self.freq_stripes_num):\n            start = np.random.randint(0, freq_bins - self.freq_drop_width)\n            spec[:, start:start+self.freq_drop_width] = 0\n        \n        return spec\n    \n\ndef spec_augment(spec: np.ndarray, num_mask=2, freq_masking_max_percentage=0.15, time_masking_max_percentage=0.1, p=0.5):\n    if random.uniform(0, 1) > p:\n        return spec\n    \n    #spec = spec.copy()\n\n    # frequency masking\n    num_freq_masks = random.randint(1, num_mask)\n    for i in range(num_freq_masks):\n        freq_percentage = random.uniform(0, freq_masking_max_percentage)\n        freq_mask_size = int(freq_percentage * spec.shape[0])\n        freq_mask_pos = random.randint(0, spec.shape[0] - freq_mask_size)\n        spec[freq_mask_pos:freq_mask_pos+freq_mask_size, :] = 0\n\n    # time masking\n    num_time_masks = random.randint(1, num_mask)\n    for i in range(num_time_masks):\n        time_percentage = random.uniform(0, time_masking_max_percentage)\n        time_mask_size = int(time_percentage * spec.shape[1])\n        time_mask_pos = random.randint(0, spec.shape[1] - time_mask_size)\n        spec[:, time_mask_pos:time_mask_pos+time_mask_size] = 0\n\n    return spec\n\n\n\nmean = (0.485, 0.456, 0.406) # RGB\nstd = (0.229, 0.224, 0.225) # RGB\nalbu_transforms = {\n    'train' : A.Compose([\n            A.Normalize(mean, std, max_pixel_value=1.0,always_apply=True),\n            A.OneOf([\n                A.Cutout(max_h_size=5, max_w_size=16),\n                A.CoarseDropout(max_holes=4),\n                #A.Lambda(image=apply_spec_augment),\n            ], p=0.5),\n            A.PadIfNeeded(min_height=Config.IMAGE_SIZE, min_width=Config.IMAGE_SIZE),\n            A.RandomCrop(width=Config.IMAGE_SIZE, height=Config.IMAGE_SIZE),      \n    ]),\n    'valid' :  A.Compose([\n                A.Normalize(mean, std, max_pixel_value=1.0,always_apply=True),\n                A.PadIfNeeded(min_height=Config.IMAGE_SIZE, min_width=Config.IMAGE_SIZE),\n                A.RandomCrop(width=Config.IMAGE_SIZE, height=Config.IMAGE_SIZE),  \n        ])\n}","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:56:53.486152Z","iopub.execute_input":"2023-04-29T00:56:53.489104Z","iopub.status.idle":"2023-04-29T00:56:53.553528Z","shell.execute_reply.started":"2023-04-29T00:56:53.489049Z","shell.execute_reply":"2023-04-29T00:56:53.551974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WaveformDataset(torch.utils.data.Dataset):\n\n    def __init__(self, df, sr = Audio.SR, duration = Audio.DURATION, chunk=Audio.CHUNK_LENGTH, train=True):\n        \n        self.df = df\n        self.sr = sr \n        self.train = train\n        self.mode = 'train' if train else 'valid'\n        self.d_len = duration * self.sr\n        self.c_len = chunk * self.sr\n        \n\n    #With nnAudio n_bins=None, the frequency bins default to n_fft/2 + 1 =513 with a 1024 window, which is optimal, but with hop lenth 512\n    #we end up with a 501x513 image, which iss too big for CUDA memory.  So downsampling is needed in the f axis by specifying n_bins\n    #513 x 313 images would also be too slow for inferance. Could consider 256x313 if that gives a performance gain.  Better still try\n    #to get a square 256x256 or 224 x 224 image\n        \n        if self.train:\n            self.wave_transforms = Compose(\n                [\n                    OneOf(\n                        [\n                            NoiseInjection(p=1, max_noise_level=0.04),\n                            GaussianNoise(p=1, min_snr=5, max_snr=20),\n                            PinkNoise(p=1, min_snr=5, max_snr=20),\n                            BrownNoise(p=1, min_snr=5, max_snr=20),\n                        ],\n                        p=0.5,\n                    ),\n                    AddBackround(p=0.6, min_snr=1, max_snr=20),\n                    Normalize(always_apply=True, p=1),\n                ]\n            )\n        else:\n            self.wave_transforms = Normalize(always_apply=True, p=1)\n        \n    def __len__(self):\n        return self.df.shape[0]\n\n    \n    def __getitem__(self, idx):    \n        row = self.df.iloc[idx]\n        wav_path = row.filepath \n        #y, _ = librosa.load(wav_path, sr=Audio.SR)\n        y, _ = torchaudio.load(wav_path)  # might need more arguments\n        y = y.squeeze().numpy()   #Faster to use torchaudio, but anyway the bottleneck is with the STFT\n        \n        if len(y) > 0: \n            y = y[:self.c_len]  \n        y = crop_or_pad(y, self.d_len, train=self.train)     \n        y = self.wave_transforms(y, sr=self.sr)\n\n        \n        image = compute_melspec(y)\n        #image = compute_pcen(y)\n        if self.train:\n            image = spec_augment(image, p=0.4, num_mask=2, freq_masking_max_percentage=0.05, time_masking_max_percentage=0.05)\n        image = reshape_image(image)\n        image = mono_to_color(image)\n        image = np.stack([image, image, image], axis=-1) #puts the chanels last, like a normal image, for the ablu_trasformations\n        image = albu_transforms[self.mode](image=image)['image']#[:,:,0]  # replace with spec_augment\n        image = image.transpose(2,0,1).astype(np.float32) # swapping the image channels to the first axis\n        targets = torch.tensor(row[2:]).float().to(torch.float32)\n        return image, targets","metadata":{"papermill":{"duration":0.039034,"end_time":"2022-04-22T06:01:17.350173","exception":false,"start_time":"2022-04-22T06:01:17.311139","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-29T00:56:53.558568Z","iopub.execute_input":"2023-04-29T00:56:53.560882Z","iopub.status.idle":"2023-04-29T00:56:53.580301Z","shell.execute_reply.started":"2023-04-29T00:56:53.560841Z","shell.execute_reply":"2023-04-29T00:56:53.579239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class InverseSqrtSampler(Sampler):\n    def __init__(self, targets_df, replacement=True):\n        targets_numeric = targets_df.select_dtypes(include='number')  # Select only numeric columns\n        self.targets = np.argmax(targets_numeric.values, axis=1)\n        self.class_counts = np.bincount(self.targets)\n        self.weights = 1.0 / np.sqrt(self.class_counts)\n        self.weights = self.weights / self.weights.sum()\n        self.indices = np.arange(len(targets_df))\n        self.replacement = replacement\n\n    def __iter__(self):\n        indices = []\n        for class_idx in range(len(self.class_counts)):\n            class_indices = self.indices[self.targets == class_idx]\n            class_sampler = torch.utils.data.sampler.WeightedRandomSampler(\n                self.weights[class_idx] * np.ones(len(class_indices)),\n                len(class_indices),\n                replacement=self.replacement\n            )\n            indices += [class_indices[i] for i in class_sampler]\n        return iter(indices)\n\n    def __len__(self):\n        return len(self.indices)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:56:53.585466Z","iopub.execute_input":"2023-04-29T00:56:53.588413Z","iopub.status.idle":"2023-04-29T00:56:53.601625Z","shell.execute_reply.started":"2023-04-29T00:56:53.588375Z","shell.execute_reply":"2023-04-29T00:56:53.600502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_fold_dataloaders(df_train, df_valid):\n        \n    ds_train = WaveformDataset(\n        df_train, \n        sr = Audio.SR,\n        duration = Audio.DURATION,\n        train = True)\n    \n    ds_val = WaveformDataset(\n        df_valid, \n        sr = Audio.SR,\n        duration = Audio.DURATION,\n        train=False)\n    \n    sampler = InverseSqrtSampler(df_train, replacement=True)\n    dl_val = DataLoader(ds_val, batch_size=Config.BATCH_SIZE, num_workers = Config.NUM_WORKERS)\n        \n    if Config.WEIGHTED_SAMPLING:\n        dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE , sampler=sampler, num_workers=Config.NUM_WORKERS)\n    else:\n        dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE , shuffle=True, num_workers=Config.NUM_WORKERS) \n        \n    return dl_train, dl_val, ds_train, ds_val","metadata":{"papermill":{"duration":0.036289,"end_time":"2022-04-22T06:01:17.539606","exception":false,"start_time":"2022-04-22T06:01:17.503317","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-29T00:56:53.607074Z","iopub.execute_input":"2023-04-29T00:56:53.61002Z","iopub.status.idle":"2023-04-29T00:56:53.621263Z","shell.execute_reply.started":"2023-04-29T00:56:53.609981Z","shell.execute_reply":"2023-04-29T00:56:53.620137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_batch(img_ds, num_rows, num_cols, predict_arr=None):\n    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, sharex=True, figsize=(15, 4*num_rows))\n    fig.tight_layout(pad=3.0)\n    img_inds = np.random.randint(0, len(img_ds)-1, num_rows*num_cols)\n    for index, ax in zip(img_inds, axes.flatten()):  # list first 9 images\n        img, lb = img_ds[index]\n        shape = img.shape\n        scaled_data = (img[0] - np.min(img[0])) / (np.max(img[0]) - np.min(img[0]))\n        img = librosa.display.specshow(scaled_data, x_axis='time', y_axis='mel', ax=ax)\n        ax.set(title= f'Spectrogram scaled to [0,1] {shape}')\n        fig.colorbar(img, ax=ax, format=\"%+2.f dB\")        ","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:56:53.626537Z","iopub.execute_input":"2023-04-29T00:56:53.629008Z","iopub.status.idle":"2023-04-29T00:56:53.641135Z","shell.execute_reply.started":"2023-04-29T00:56:53.62897Z","shell.execute_reply":"2023-04-29T00:56:53.639333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dl_train, dl_val, ds_train, ds_val = get_fold_dataloaders(train_df, val_df)\nshow_batch(ds_train, 4, 2)","metadata":{"papermill":{"duration":0.584852,"end_time":"2022-04-22T06:01:18.338238","exception":false,"start_time":"2022-04-22T06:01:17.753386","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-29T00:56:53.646677Z","iopub.execute_input":"2023-04-29T00:56:53.64951Z","iopub.status.idle":"2023-04-29T00:57:08.666434Z","shell.execute_reply.started":"2023-04-29T00:56:53.649471Z","shell.execute_reply":"2023-04-29T00:57:08.665029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_optimizer(lr, params):\n    model_optimizer = torch.optim.Adam(\n            filter(lambda p: p.requires_grad, params), \n            lr=lr,\n            weight_decay=Config.WEIGHT_DECAY )\n    interval = \"epoch\"\n    \n    lr_scheduler = CosineAnnealingWarmRestarts(\n                            model_optimizer, \n                            T_0=Config.EPOCHS, \n                            T_mult=1, \n                            eta_min=1e-6, \n                            last_epoch=-1)\n\n    return { \"optimizer\": model_optimizer, \n             \"lr_scheduler\": {\"scheduler\": lr_scheduler,\n                        \"interval\": interval,\n                        \"monitor\": \"val_loss\",\n                        \"frequency\": 1}}","metadata":{"papermill":{"duration":0.048043,"end_time":"2022-04-22T06:01:22.109544","exception":false,"start_time":"2022-04-22T06:01:22.061501","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-29T00:57:08.667794Z","iopub.execute_input":"2023-04-29T00:57:08.66888Z","iopub.status.idle":"2023-04-29T00:57:08.676679Z","shell.execute_reply.started":"2023-04-29T00:57:08.668822Z","shell.execute_reply":"2023-04-29T00:57:08.67579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def padded_cmap(solution, submission, padding_factor=5):\n    solution = solution.fillna(0).replace([pd.np.inf, -pd.np.inf], 0)\n    submission = submission.fillna(0).replace([pd.np.inf, -pd.np.inf], 0)\n    new_rows = []\n    for i in range(padding_factor):\n        new_rows.append([1 for i in range(len(solution.columns))])\n    new_rows = pd.DataFrame(new_rows)\n    new_rows.columns = solution.columns\n    padded_solution = pd.concat([solution, new_rows]).reset_index(drop=True).copy()\n    padded_submission = pd.concat([submission, new_rows]).reset_index(drop=True).copy()\n    score = skm.average_precision_score(\n        padded_solution.values,\n        padded_submission.values,\n        average='macro')    \n    return score\n\n\ndef map_score(solution, submission):\n    solution = solution.fillna(0).replace([pd.np.inf, -pd.np.inf], 0)\n    submission = submission.fillna(0).replace([pd.np.inf, -pd.np.inf], 0)\n    score = skm.average_precision_score(\n        solution.values,\n        submission.values,\n        average='micro')  \n    return score\n\n\ndef mixup_data(x, y, alpha=1.0, device=device):\n    '''Returns mixed inputs, pairs of targets, and lambda'''\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).to(device)\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:57:08.678248Z","iopub.execute_input":"2023-04-29T00:57:08.679118Z","iopub.status.idle":"2023-04-29T00:57:08.696874Z","shell.execute_reply.started":"2023-04-29T00:57:08.679048Z","shell.execute_reply":"2023-04-29T00:57:08.695928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing Validation Strategy\ndummy = val_df[birds].copy()\ndummy[birds] = np.random.rand(dummy.shape[0],dummy.shape[1])\npad_5 = padded_cmap(val_df[birds], dummy[birds], padding_factor = 5)\npad_1 = padded_cmap(val_df[birds], dummy[birds], padding_factor = 1)\n\nprint(f'Padded cMAP, with padding=5: {pad_5}\\n'\n     f'Padded cMAP, with padding=1: {pad_1}\\n'\n     f'MAP score: {map_score(val_df[birds], dummy[birds])}')","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:57:08.698771Z","iopub.execute_input":"2023-04-29T00:57:08.699515Z","iopub.status.idle":"2023-04-29T00:57:10.352292Z","shell.execute_reply.started":"2023-04-29T00:57:08.69948Z","shell.execute_reply":"2023-04-29T00:57:10.351146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClassifierHead(nn.Module):\n    def __init__(self, num_classes, num_features, dropout_rate=0.2):\n        super().__init__()\n        \n        self.dense_layer = nn.Linear(num_features, num_features)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout(p=dropout_rate)  #removed in experiment 22\n        self.output_layer = nn.Linear(num_features, num_classes)\n        \n    def forward(self, x):\n        x = self.dense_layer(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.output_layer(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:57:10.354179Z","iopub.execute_input":"2023-04-29T00:57:10.354951Z","iopub.status.idle":"2023-04-29T00:57:10.362904Z","shell.execute_reply.started":"2023-04-29T00:57:10.354908Z","shell.execute_reply":"2023-04-29T00:57:10.361655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BirdClefModel(pl.LightningModule):\n    def __init__(self, model_name=Config.MODEL, num_classes = Config.NUM_CLASSES, pretrained = Config.PRETRAINED, loss=Config.LOSS_FUNCTION):\n        super().__init__()\n        self.num_classes = num_classes\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        self.in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Sequential(nn.Linear(self.in_features, num_classes))\n        #self.backbone.classifier = ClassifierHead(num_classes, self.in_features)\n        self.loss_function = loss\n        self.val_outputs = []\n        self.train_outputs = []\n        self.metrics = []\n\n    def forward(self,images):\n        x = self.backbone(images)\n        return x\n        \n    def configure_optimizers(self):\n        return get_optimizer(lr=Config.LR, params=self.parameters())\n\n    def train_with_mixup(self, X, y):\n        X, y_a, y_b, lam = mixup_data(X, y, alpha=Config.MIXUP_ALPHA)\n        y_pred = self(X)\n        loss_mixup = mixup_criterion(self.loss_function, y_pred, y_a, y_b, lam)\n        return loss_mixup\n\n    def training_step(self, batch, batch_idx):\n        image, target = batch        \n        if Config.USE_MIXUP:\n            loss = self.train_with_mixup(image, target)\n        else:\n            y_pred = self(image)\n            loss = self.loss_function(y_pred,target) \n        self.train_outputs.append({'train_loss': loss})\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n        return loss        \n\n    def validation_step(self, batch, batch_idx):\n        image, target = batch     \n        y_pred = self(image)\n        val_loss = self.loss_function(y_pred, target)\n        self.log(\"val_loss\", val_loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n        outputs = {\"val_loss\": val_loss, \"logits\": y_pred, \"targets\": target}\n        self.val_outputs.append(outputs)\n        return outputs\n    \n    def train_dataloader(self):\n        return self._train_dataloader \n    \n    def validation_dataloader(self):\n        return self._validation_dataloader\n    \n    def on_validation_epoch_end(self):\n        outputs = self.val_outputs\n        train_outputs = self.train_outputs\n        train_losses = [x['train_loss'] for x in train_outputs]\n        avg_train_loss = sum(train_losses) / len(train_losses) if train_losses else 0.0\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        output_logits = torch.cat([x['logits'] for x in outputs],dim=0)\n        \n        if Config.LOSS_FUNCTION == nn.CrossEntropyLoss():\n            output_probs = F.softmax(output_logits, dim=1).cpu().detach().numpy()\n        else:\n            output_probs = output_logits.sigmoid().cpu().detach().numpy()\n  \n        target_val = torch.cat([x['targets'] for x in outputs],dim=0).cpu().detach().numpy()  \n        pred_df = pd.DataFrame(output_probs, columns = birds)\n        val_df = pd.DataFrame(target_val, columns = birds)\n       \n        avg_score = padded_cmap(val_df, pred_df, padding_factor = 5)\n        avg_score2 = padded_cmap(val_df, pred_df, padding_factor = 3)\n        avg_score3 = skm.label_ranking_average_precision_score(target_val,output_probs)\n        self.metrics.append({'train_loss':avg_train_loss, 'val_loss': avg_loss, 'map':avg_score3, 'map5':avg_score})\n        \n        # competition_metrics(output_val,target_val)\n        print(f'epoch {self.current_epoch} train loss {avg_train_loss}')\n        print(f'epoch {self.current_epoch} validation loss {avg_loss}')\n        print(f'epoch {self.current_epoch} validation C-MAP score pad 5 {avg_score}')\n        print(f'epoch {self.current_epoch} validation C-MAP score pad 3 {avg_score2}')\n        print(f'epoch {self.current_epoch} validation AP score {avg_score3}')\n        \n        val_df.to_pickle('val_df.pkl')\n        pred_df.to_pickle('pred_df.pkl')    \n        return self.metrics","metadata":{"papermill":{"duration":0.156714,"end_time":"2022-04-22T06:01:22.301564","exception":false,"start_time":"2022-04-22T06:01:22.14485","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-29T00:57:10.365005Z","iopub.execute_input":"2023-04-29T00:57:10.365402Z","iopub.status.idle":"2023-04-29T00:57:10.387788Z","shell.execute_reply.started":"2023-04-29T00:57:10.365359Z","shell.execute_reply":"2023-04-29T00:57:10.386541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_training():\n    val_loss=[]\n    train_loss=[]\n    print(f\"Running training...\")\n    logger = None\n    dl_train, dl_val, ds_train, ds_val = get_fold_dataloaders(train_df, val_df)\n    audio_model = BirdClefModel()\n\n    early_stop_callback = EarlyStopping(monitor=\"val_loss\", \n                                        min_delta=Config.MIN_DELTA, \n                                        patience=Config.PATIENCE, \n                                        verbose= True, \n                                        mode=\"min\")\n    \n    # saves top- checkpoints based on \"val_loss\" metric\n    checkpoint_callback = ModelCheckpoint(save_top_k=6,\n                                          monitor=\"val_loss\",\n                                          mode=\"min\",\n                                          dirpath=save_path,\n                                          save_last= True,\n                                          save_weights_only=True, \n                                          verbose= True,\n                                          #filename=f'birdCLEF23-{trainer.current_epoch:02d}-{val_loss:.4f}',  #need to figure this out so It can update to a dataset\n)\n    \n      \n    callbacks_to_use = [checkpoint_callback, early_stop_callback, ]\n\n    trainer = Trainer(\n        gpus=1,\n        val_check_interval=0.5,\n        deterministic=True,\n        max_epochs=Config.EPOCHS,\n        logger=logger,\n        auto_lr_find=False,    \n        callbacks=callbacks_to_use,\n        precision=Config.PRECISION, accelerator=accelerator)\n\n    \n    print(\"Running trainer.fit\")\n    trainer.fit(audio_model, train_dataloaders = dl_train, val_dataloaders = dl_val)       \n    training_results = trainer.callback_metrics['validation_epoch_end']\n    \n    gc.collect()\n    torch.cuda.empty_cache()\n    return training_results","metadata":{"papermill":{"duration":0.052364,"end_time":"2022-04-22T06:01:22.708806","exception":false,"start_time":"2022-04-22T06:01:22.656442","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-29T00:58:44.896962Z","iopub.execute_input":"2023-04-29T00:58:44.897672Z","iopub.status.idle":"2023-04-29T00:58:44.908606Z","shell.execute_reply.started":"2023-04-29T00:58:44.897633Z","shell.execute_reply":"2023-04-29T00:58:44.907157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = run_training()","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:58:51.468078Z","iopub.execute_input":"2023-04-29T00:58:51.468804Z","iopub.status.idle":"2023-04-29T01:00:08.517743Z","shell.execute_reply.started":"2023-04-29T00:58:51.468767Z","shell.execute_reply":"2023-04-29T01:00:08.513338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses = [x['train_loss'] for x in metrics]\nval_losses = [x['val_loss'] for x in metrics]  \nval_map = [x['map'] for x in metrics]  \nval_map5 = [x['map5'] for x in metrics]  \n\n# Plot the training and validation losses\nplt.plot(train_losses, color='b', label='Train Loss')\nplt.plot(val_losses, color='g', label='Val Loss')\n\n# Set the x-axis label and the label for the first y-axis\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n\n# Add a second y-axis for the validation accuracy\nax2 = plt.twinx()\nax2.plot(val_accs, color='r', label='Val mAP')\nax2.plot(val_accs, color='o', label='Val mAP-5')\nax2.set_ylabel('Accuracy')\n\n# Set the legend for the plot\nplt.legend(loc='upper right')\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:57:14.960322Z","iopub.status.idle":"2023-04-29T00:57:14.961272Z","shell.execute_reply.started":"2023-04-29T00:57:14.96099Z","shell.execute_reply":"2023-04-29T00:57:14.961016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pred = pd.read_pickle('/kaggle/working/pred_df.pkl')\ndf_true = pd.read_pickle('/kaggle/working/val_df.pkl')\npadded_cmap(df_true, df_pred, padding_factor = 5)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T00:57:14.962696Z","iopub.status.idle":"2023-04-29T00:57:14.963754Z","shell.execute_reply.started":"2023-04-29T00:57:14.963448Z","shell.execute_reply":"2023-04-29T00:57:14.963476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def padded_cmap_by_class(solution, submission, padding_factor=5):\n    solution = solution.fillna(0).replace([np.inf, -np.inf], 0)\n    submission = submission.fillna(0).replace([np.inf, -np.inf], 0)\n    new_rows = []\n    for i in range(padding_factor):\n        new_rows.append([1 for i in range(len(solution.columns))])\n    new_rows = pd.DataFrame(new_rows)\n    new_rows.columns = solution.columns\n    padded_solution = pd.concat([solution, new_rows]).reset_index(drop=True).copy()\n    padded_submission = pd.concat([submission, new_rows]).reset_index(drop=True).copy()\n    \n    column_headers = list(solution.columns)\n    scores = {}\n    \n    for column in column_headers:\n        score = skm.average_precision_score(\n            padded_solution[[column]].values,\n            padded_submission[[column]].values,\n            average='macro')    \n        scores[column] = score\n    return scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cmap5_by_class = padded_cmap_by_class(df_true, df_pred, padding_factor=5)\n#cmap5_by_class  # A dict with name:score\nnp.mean(list(cmap5_by_class.values())) #checking it's the same thing","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute the frequency count of each class in the target dataframe\ncol_sums = [(col, df_true[col].sum()) for col in df_true.columns]\nnames_by_frequency = sorted(col_sums, key=lambda x: x[1], reverse=True)\n\n# extract names and counts as separate lists\nnames = [name for name, _ in names_by_frequency]\ncounts = [count for _, count in names_by_frequency]\nscores = [cmap5_by_class[name] for name in names]\n\n# Create a dataframe\ndf = pd.DataFrame({'names': names, 'counts': counts, 'scores': scores})\ndf[\"scores\"] = pd.to_numeric(df[\"scores\"])\ndf[\"counts\"] = pd.to_numeric(df[\"counts\"])","metadata":{"execution":{"iopub.status.busy":"2023-04-29T03:26:19.335918Z","iopub.execute_input":"2023-04-29T03:26:19.336378Z","iopub.status.idle":"2023-04-29T03:26:19.437978Z","shell.execute_reply.started":"2023-04-29T03:26:19.336335Z","shell.execute_reply":"2023-04-29T03:26:19.435978Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/984412645.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compute the frequency count of each class in the target dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcol_sums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnames_by_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_sums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df_target' is not defined"],"ename":"NameError","evalue":"name 'df_target' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Set the default renderer to 'notebook'\npio.renderers.default = 'notebook'\nfig = px.bar(df, x='scores', y='names', color='counts', orientation='h', hover_data=['counts', 'scores'], range_x=[0.5, 1])\nfig.update_layout(height=1200)\n\n# Show the plot\nfig.show()","metadata":{},"execution_count":null,"outputs":[]}]}